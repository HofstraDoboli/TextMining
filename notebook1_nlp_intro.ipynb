{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HofstraDoboli/TextMining_F22/blob/main/notebook1_nlp_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mARFkJ5Pwtf9",
        "outputId": "9c537e8f-bf53-497e-8308-d3f8b6f753f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"Why didn't she text me back yet?\n",
            "2 She doesn't like me anymore!\"\n",
            "3 There's no way I'm trying out for the team.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "\n",
        "text1 = '''\"Why didn't she text me back yet? She doesn't like me anymore!\" There's no way I'm trying out for the team.'''\n",
        "\n",
        "sents = sent_tokenize(text1)\n",
        "\n",
        "for i, s in enumerate(sents):\n",
        "  print(i+1, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "P0PHC8o7wtgB",
        "outputId": "bb368285-0eba-49e4-f93d-d1d89e6331f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['', 'Why', 'didn', 't', 'she', 'text', 'me', 'back', 'yet', '', 'She', 'doesn', 't', 'like', 'me', 'anymore', '', '', 'There', 's', 'no', 'way', 'I', 'm', 'trying', 'out', 'for', 'the', 'team', '']\n"
        }
      ],
      "source": [
        "import re\n",
        "words = re.split(r'[.,!? \"\\']', text1)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEqb0t3RwtgC",
        "outputId": "bc2db2c7-f32d-4a1b-bf87-f978d6c7904a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 USB4 is done, the group developing the next version of the immensely successful USB connector technology said Tuesday.\n",
            "2 USB4 doubles speeds compared to today's fastest USB 3.2 by incorporating Intel's speedy Thunderbolt technology that you already see on high-end laptops and peripherals.\n",
            "3 The USB Implementers Forum announced the completion of the technical specification Tuesday, a move that frees hardware and software engineers to get cracking building the actual products to support it.\n",
            "4 Today's USB 3.2, which enables data transfer speeds up to 20 gigabits per second, is still something of a rarity; most of us have earlier versions of the technology that works at 5Gbps or 10Gbps.\n",
            "5 USB4 promises a speed boost to 40Gbps, helpful for things like using multiple external displays or fetching files from external hard drives.\n"
          ]
        }
      ],
      "source": [
        "text2 = '''USB4 is done, the group developing the next version of the immensely successful USB connector technology said Tuesday. USB4 doubles speeds compared to today's fastest USB 3.2 by incorporating Intel's speedy Thunderbolt technology that you already see on high-end laptops and peripherals. The USB Implementers Forum announced the completion of the technical specification Tuesday, a move that frees hardware and software engineers to get cracking building the actual products to support it.\n",
        "\n",
        "Today's USB 3.2, which enables data transfer speeds up to 20 gigabits per second, is still something of a rarity; most of us have earlier versions of the technology that works at 5Gbps or 10Gbps. USB4 promises a speed boost to 40Gbps, helpful for things like using multiple external displays or fetching files from external hard drives.'''\n",
        "sents2= sent_tokenize(text2)\n",
        "for i, s in enumerate(sents2):\n",
        "  print(i+1, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMmoqP6DwtgC",
        "outputId": "f3b30596-e1a3-4e86-ed61-cfe0ce76a7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'Why', 'did', \"n't\", 'she', 'text', 'me', 'back', 'yet', '?', 'She', 'does', \"n't\", 'like', 'me', 'anymore', '!', \"''\", 'There', \"'s\", 'no', 'way', 'I', \"'m\", 'trying', 'out', 'for', 'the', 'team', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "words = word_tokenize(text1)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPbeuyuiwtgD",
        "outputId": "88c427b8-7956-4fb8-f299-486836d56671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['Why', 'did', \"n't\", 'she', 'text', 'me', 'back', 'yet', 'She', 'does', \"n't\", 'like', 'me', 'anymore', 'There', 'no', 'way', 'I', 'trying', 'out', 'for', 'the', 'team']\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "print(punctuation)\n",
        "\n",
        "words_no_punkt = [w for w in words if w[0] not in punctuation] # list comprehension\n",
        "'''l = []\n",
        "for w in words:\n",
        "    if w[0] not in punctuation:\n",
        "        l.append(w)'''\n",
        "print(words_no_punkt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO2NTiAnwtgE",
        "outputId": "cf18a63d-2172-4894-9910-fb5d4754ccf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# words lowercase 23\n",
            "['why', 'did', \"n't\", 'she', 'text', 'me', 'back', 'yet', 'she', 'does', \"n't\", 'like', 'me', 'anymore', 'there', 'no', 'way', 'i', 'trying', 'out', 'for', 'the', 'team']\n"
          ]
        }
      ],
      "source": [
        "# lower case all words\n",
        "words_lower = [w.lower() for w in words_no_punkt]\n",
        "print(\"# words lowercase\", len(words_lower))\n",
        "print(words_lower)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(\"number stopwords\", len(stopwords.words(\"english\")))\n",
        "print(stopwords.words(\"english\"))\n",
        "words_no_stop = [w for w in words_lower if w not in stopwords.words(\"english\")]\n",
        "print('# words no stopwords', len(words_no_stop))\n",
        "print(words_no_stop)"
      ],
      "metadata": {
        "id": "mvC_-i_SBwGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd5d8d1-a469-477f-c3b9-a6cd30e03976"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number stopwords 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "# words no stopwords 10\n",
            "[\"n't\", 'text', 'back', 'yet', \"n't\", 'like', 'anymore', 'way', 'trying', 'team']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHwSSJy5wtgG",
        "outputId": "b4fbc5fa-9822-4415-dc12-7c65e80268b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('``', '``'), ('Why', 'WRB'), ('did', 'VBD'), (\"n't\", 'RB'), ('she', 'PRP'), ('text', 'VB'), ('me', 'PRP'), ('back', 'RB'), ('yet', 'RB'), ('?', '.')]\n",
            "[('She', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('like', 'VB'), ('me', 'PRP'), ('anymore', 'RB'), ('!', '.'), (\"''\", \"''\")]\n",
            "[('There', 'EX'), (\"'s\", 'VBZ'), ('no', 'DT'), ('way', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('trying', 'VBG'), ('out', 'RP'), ('for', 'IN'), ('the', 'DT'), ('team', 'NN'), ('.', '.')]\n",
            "[[('``', '``'), ('Why', 'WRB'), ('did', 'VBD'), (\"n't\", 'RB'), ('she', 'PRP'), ('text', 'VB'), ('me', 'PRP'), ('back', 'RB'), ('yet', 'RB'), ('?', '.')], [('She', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('like', 'VB'), ('me', 'PRP'), ('anymore', 'RB'), ('!', '.'), (\"''\", \"''\")], [('There', 'EX'), (\"'s\", 'VBZ'), ('no', 'DT'), ('way', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('trying', 'VBG'), ('out', 'RP'), ('for', 'IN'), ('the', 'DT'), ('team', 'NN'), ('.', '.')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# attach parts of speech - for words in each sentence \n",
        "from nltk import pos_tag\n",
        "download('averaged_perceptron_tagger')\n",
        "\n",
        "words_pos = []\n",
        "for s in sents: # for each sentence\n",
        "  words_s = word_tokenize(s)\n",
        "  pos_sents = pos_tag(words_s) # extract parts of speech\n",
        "  print(pos_sents)\n",
        "  words_pos.append(pos_sents) # different than append \n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Ey3AhMaOwtgH",
        "outputId": "3ebaf4a3-e796-4d77-cc08-1186c40b1edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['did', \"n't\", 'text', 'back', 'yet', 'does', \"n't\", 'like', 'anymore', 'way', 'i', 'trying', 'out', 'team']\n"
        }
      ],
      "source": [
        "# eliminate all words that are not nouns, verbs, adj or adverbs\n",
        "string_keep = 'NVRJ'\n",
        "words_nvrj = [w for (w,pos) in words_pos if pos[0] in string_keep]\n",
        "print(words_nvrj)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show stemming - cuts parts of words that vary with part of speech or tense (Verb)\n",
        "from nltk.stem import *\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = stemmer.stem"
      ],
      "metadata": {
        "id": "Dv1lQYJgB3gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lo7PoaHwtgH"
      },
      "outputs": [],
      "source": [
        "# show lemmatization "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}