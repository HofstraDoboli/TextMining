{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "gpuClass": "premium",
      "authorship_tag": "ABX9TyPkdMXGUZeYV8wmRxkBBzXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HofstraDoboli/TextMining/blob/main/bert_sentiment_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSiybAbjviym"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets --upgrade\n",
        "!pip install transformers[torch]\n",
        "!pip install evaluate\n",
        "! pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhPFo76qvxT3",
        "outputId": "1028d3d3-8e7e-425f-923d-ecd80a551746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/TextMining/DataSets/Covid_Tweets/\n",
        "%ls *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNoKro9wgWn",
        "outputId": "2e853ebe-1188-420b-b085-5e7bd955b23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TextMining/DataSets/Covid_Tweets\n",
            "Corona_NLP_test.csv  Corona_NLP_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# download the data from\": https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification\n",
        "dataset = load_dataset('csv', data_files={'train': 'Corona_NLP_train.csv', 'test': 'Corona_NLP_test.csv'}, encoding = \"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "Ya8Tk9-av7Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documentation datasets https://huggingface.co/docs/datasets/index\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3u-0eWBwsp5",
        "outputId": "b45bd37a-1488-4552-880a-b46821e158fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment'],\n",
              "        num_rows: 41157\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment'],\n",
              "        num_rows: 3798\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][:5] # ['OriginalTweet']"
      ],
      "metadata": {
        "id": "lCqj27Zfxx4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Tokenize data\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "# PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:128\n",
        "base_model = 'distilbert-base-cased' # \"bert-base-cased\" # distilbert-base-cased - smaller model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model) # cased = means it was trained to recognize capitalization (vs. bert-base-uncased)"
      ],
      "metadata": {
        "id": "32QuWPnwx-QE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007743ab-be7d-4c81-f8d7-c316e70b64e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrAc-rOJPoee",
        "outputId": "c3485961-efbb-45c3-a672-3f1e3ad20352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exaample what tokenization does\n",
        "print(dataset['train'][10][\"OriginalTweet\"])\n",
        "print(len(dataset['train'][10][\"OriginalTweet\"].split(' ')))\n",
        "test_token = tokenizer(dataset['train'][10][\"OriginalTweet\"])\n",
        "print(test_token.keys())\n",
        "print(len(test_token['input_ids']))\n",
        "print(test_token['input_ids'])\n",
        "\n",
        "# input_ids = token's ids for parts of words in the text\n",
        "# token_type_ids = all 0's if the input is one texr, 0's followed by 1's if input is a two sentence text\n",
        "# attention_mask = if any of the words should be masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h95o_PKN009c",
        "outputId": "b4ba3018-033d-4773-b0aa-25084cf05aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All month there hasn't been crowding in the supermarkets or restaurants, however reducing all the hours and closing the malls means everyone is now using the same entrance and dependent on a single supermarket. #manila #lockdown #covid2019 #Philippines https://t.co/HxWs9LAnF9\n",
            "39\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "73\n",
            "[101, 1398, 2370, 1175, 8186, 112, 189, 1151, 3515, 1158, 1107, 1103, 20247, 1116, 1137, 7724, 117, 1649, 7914, 1155, 1103, 2005, 1105, 5134, 1103, 8796, 1116, 2086, 2490, 1110, 1208, 1606, 1103, 1269, 3448, 1105, 7449, 1113, 170, 1423, 20247, 119, 108, 1299, 8009, 108, 5842, 5455, 108, 1884, 18312, 10973, 16382, 108, 4336, 18630, 131, 120, 120, 189, 119, 1884, 120, 145, 1775, 2924, 1116, 1580, 10783, 1179, 2271, 1580, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize all dataset - this works only with padding = True or padding = 'max_length'\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['OriginalTweet'], truncation = True, padding = True, return_tensors = \"pt\").to(device) # padding='max_length'\n",
        "\n",
        "dataset = dataset.map(tokenize_data, batched = True)  # tokenizes all the data"
      ],
      "metadata": {
        "id": "nxYE3FPp07Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset['train'][1]['input_ids'])) # attention_mask is 1 for non padding tokens only, 0 for padding tokens\n",
        "print(len(dataset['train'][1]['input_ids']))"
      ],
      "metadata": {
        "id": "c16BLew434Bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d45a5b-4909-43b0-937e-126fea78bd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH9dHpCxHX4l",
        "outputId": "c7dbd88c-2bed-4ed2-b973-59d6a28f35e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 41157\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 3798\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the labels from words to numbers\n",
        "def labels2num(label):\n",
        "\n",
        "    label = label['Sentiment']\n",
        "    dict_labels = {'Positive':0, 'Negative':1, 'Neutral':2,\n",
        "                   'Extremely Positive':3, 'Extremely Negative':4}\n",
        "    return {'labels': dict_labels[label]}\n",
        "\n",
        "print(labels2num(dataset['train'][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAsy5-e6191a",
        "outputId": "6bf7bfb5-3f5c-4997-c71e-19fdbf91b026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'labels': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_columns = ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']\n",
        "dataset = dataset.map(labels2num, remove_columns=remove_columns)"
      ],
      "metadata": {
        "id": "ssvWHKbZ19LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset # see it added 'labels' feature and removed the rest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFhnVCNw3RMc",
        "outputId": "66f3cc5a-e18e-4ed0-8f0e-dc34ad047fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 41157\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3798\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uQZbq9ALjAjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset['train']['input_ids'][1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ5WmfWLDx9R",
        "outputId": "e8beb930-d163-4d37-f577-d0aee700e7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set format of the dataset to torch\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\",  \"attention_mask\", \"labels\"]) # \"token_type_ids\",\n",
        "dataset['train'].format['type']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6FADsL_h4I3Z",
        "outputId": "24bed369-3332-4f08-d804-09d6fc778def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training and validation datesets\n",
        "train_dataset = dataset['train'].shuffle(seed=10).select(range(40000))\n",
        "eval_dataset  = dataset['train'].shuffle(seed=10).select(range(40000, 41000)) # validation dataset"
      ],
      "metadata": {
        "id": "nnUSFfBn8Tf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator, DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer) # only if you did not pad the data with the tokenizer"
      ],
      "metadata": {
        "id": "mqan6IwOGl12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training arguments, trainer, AutoModel\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "#data_collator = DataCollatorWithPadding(tokenizer) # use only if you did not pad the data with the tokenizer\n",
        "# base_model = \"bert-base-cased\" # already defined above\n",
        "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels = 5).to(device)\n"
      ],
      "metadata": {
        "id": "gZRuKdZn3m_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839bcae6-495b-4b4a-93d3-d47e4efb9257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters)"
      ],
      "metadata": {
        "id": "lQnkYGvHft0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d98415d-e9ba-4306-d964-a291ae0d7e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.parameters of DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir  = 'OutputTrain', # output directory\n",
        "    num_train_epochs = 3,\n",
        "    eval_strategy = \"epoch\",\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = 64,\n",
        "    per_device_eval_batch_size = 64,\n",
        "    weight_decay = 0.01,\n",
        "    push_to_hub = False,\n",
        "    report_to = None\n",
        ")\n",
        "# training_args\n",
        "import wandb # a library to visualize training process and results - disable it\n",
        "wandb.init(mode='disabled')"
      ],
      "metadata": {
        "id": "1yt9KxwP7pLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "10267445-b689-4f78-ff4b-2e45cd6e9a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dfecc5a37c0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyHK6rTvIpRv",
        "outputId": "af39ae26-0439-4ea7-83f1-da4c02210513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=epoch,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=OutputTrain/runs/Nov21_19-10-58_75ad090c212e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=OutputTrain,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=OutputTrain,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1:3]['labels'].shape # 'input_ids'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9IryCkZFvnJ",
        "outputId": "e6be570a-1693-4c36-d78c-5af1474fac22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metrics = {}\n",
        "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
        "    metrics.update(precision.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "    metrics.update(recall.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "    metrics.update(f1.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "jsuRDP6_8sCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model= model,\n",
        "    args = training_args,\n",
        "    train_dataset= train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    data_collator= data_collator, # only if you want to pad inputs during training\n",
        "    compute_metrics = compute_metrics,\n",
        "    # tokenizer = tokenizer - no longer used\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "kc3BTbsh8iFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test compute_metrics\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "MnSiUM38zL-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml\n",
        "from pynvml import *\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxtFQJpdz2RV",
        "outputId": "61d503d7-b7c1-4dd2-8d44-c5f563346158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (11.5.3)\n",
            "GPU memory occupied: 5985 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # it does batch size - 1875 * 64 = 120000/3 epochs = 40K samples per epoch"
      ],
      "metadata": {
        "id": "778jq91YAh8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check results of the trained model\n",
        "trainer.evaluate(dataset['test'])"
      ],
      "metadata": {
        "id": "pLgXxTK6Qw5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label = {0: 'Positive', 1: 'Negative', 2: 'Neutral', 3: 'Extremely Positive', 4: 'Extremely Negative'}\n",
        "model.config.label2id = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'Extremely Positive': 3, 'Extremely Negative': 4}"
      ],
      "metadata": {
        "id": "JuCHjLjyT0KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model after training\n",
        "model.save_pretrained(\"OutputTrain/model_11_21_24\")\n",
        "tokenizer.save_pretrained(\"OutputTrain/tokenizer_11_21_24\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUDN8z_bQzTf",
        "outputId": "ec7a472e-af5b-467a-fd4e-a0874228f74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('OutputTrain/tokenizer_11_21_24/tokenizer_config.json',\n",
              " 'OutputTrain/tokenizer_11_21_24/special_tokens_map.json',\n",
              " 'OutputTrain/tokenizer_11_21_24/vocab.txt',\n",
              " 'OutputTrain/tokenizer_11_21_24/added_tokens.json',\n",
              " 'OutputTrain/tokenizer_11_21_24/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RXRNQJ1fSIjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best_model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model     = AutoModelForSequenceClassification.from_pretrained(\"OutputTrain/model_11_21_24\").to(device)\n",
        "best_tokenizer = AutoTokenizer.from_pretrained(\"OutputTrain/tokenizer_11_21_24\")"
      ],
      "metadata": {
        "id": "C5XntyjHRsFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# download the data from\": https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification\n",
        "dataset_test = load_dataset('csv', data_files={'test': 'Corona_NLP_test.csv'}, encoding = \"ISO-8859-1\")\n",
        "print(dataset_test)\n"
      ],
      "metadata": {
        "id": "meUy7lxcSyMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    inputs = best_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = best_model(**inputs).logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    return best_model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "usPzylEgrCby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS3BukQaTcee",
        "outputId": "80a7e7ce-82a5-4767-8000-6cc605382549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Positive',\n",
              " 1: 'Negative',\n",
              " 2: 'Neutral',\n",
              " 3: 'Extremely Positive',\n",
              " 4: 'Extremely Negative'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_tweet = 10\n",
        "tweet = dataset_test['test']['OriginalTweet'][ind_tweet]\n",
        "print(tweet)\n",
        "print('Predicted sentiment', predict(tweet))\n",
        "print('Original sentiment', dataset_test['test']['Sentiment'][ind_tweet])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5PbXut9QHBW",
        "outputId": "d3ceee6b-216b-419f-c71e-a5a385162ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best quality couches at unbelievably low prices available to order.\r\r\n",
            "\r\r\n",
            "We are in Boksburg GP \r\r\n",
            "\r\r\n",
            "For more info WhatsApp:\r\r\n",
            "084 764 8086\r\r\n",
            "\r\r\n",
            "#SuperTuesdsy #PowerTalk \r\r\n",
            "#Covid_19 #SayEntrepreneur \r\r\n",
            "#DJSBU https://t.co/HhDJhyQ2Dc\n",
            "Predicted sentiment Extremely Positive\n",
            "Original sentiment Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==== UP TO HERE 11/20/24 ============"
      ],
      "metadata": {
        "id": "MdS4SQQqU9Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train more\n",
        "\n",
        "trainer.train(sume_from_checkpoint = \"OutputTrain/checkpoint-1875\")"
      ],
      "metadata": {
        "id": "ET8C6LxSSF6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Z4FYX2bLJriZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}