{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMVJL21gHhIcyR2KrMhtX9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HofstraDoboli/TextMining_F22/blob/main/indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bf1PEG2uNrNa"
      },
      "outputs": [],
      "source": [
        "list_doc =[ \"Information retrieval course will cover algorithms used in search engines for finding relevant documents or information related to a query. Topics include: natural language processing for extracting relevant terms out of text data, vector space a methods for computing similarity between documents, text classification, and clustering. These techniques are commonly used in applications such as: automatic extraction of summaries out of a long text, extract novel information in a stream of data.\",  \n",
        "\n",
        "\"NLP algorithms are typically based on machine learning algorithms. Instead of hand-coding large sets of rules, NLP can rely on machine learning to automatically learn these rules by analyzing a set of examples (i.e. a large corpus, like a book, down to a collection of sentences), and making a statical inference. In general, the more data analyzed, the more accurate the model will be.\",  \n",
        "\n",
        "\"The Denver Broncos made sure Brandon McManus will be their kicker for the long haul on Monday.  General manager John Elway announced the team and the kicker agreed on a contract extension. NFL Network Insider Ian Rapoport reported, per a source, it's a three-year extension worth $11.254 million with $6 million of it guaranteed. McManus is now the NFL's fourth highest paid kicker.\", \n",
        "\n",
        " \"Equifax, one of the three major credit reporting agencies, handles the data of 820 million consumers and more than 91 million businesses worldwide. Between May and July of this year 143 million people in the U.S. may have had their names, Social Security numbers, birth dates, addresses and even driver's license numbers accessed. In addition, the hack compromised 209,000 people's credit card numbers and personal dispute details for another 182,000 people. What bad actors could do with that information is daunting. This data breach is more confusing than others -- like when Yahoo or Target were hacked, for example -- according to Joel Winston, a former deputy attorney general for New Jersey , whose current law practice focuses on consumer rights litigation, information privacy, and data protection law.\", \n",
        " \n",
        "  \"\"\"Why didn't she text me back yet? She doesn't like me anymore!\" \"There's no way I'm trying out for the team. I suck at basketball\"\"It's not fair that I have a curfew! \"Sound familiar? Parents of tweens and teens often shrug off such anxious and gloomy thinking as normal irritability and moodiness â€” because it is. Still, the beginning of a new school year, with all of the required adjustments, is a good time to consider just how closely the habit of negative, exaggerated \"self-talk\" can affect academic and social success, self-esteem and happiness. Psychological research shows that what we think can have a powerful influence on how we feel emotionally and physically, and on how we behave. Research also shows that our harmful thinking patterns can be changed.\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy   # another tokenizer, lemmatizer (has --> be)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.disable_pipes('parser', 'ner')  \n"
      ],
      "metadata": {
        "id": "xO9TCnt1OLMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dff2cb-58c9-4b3e-f2e9-1b8c8e2835f3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parser', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: text processing for one document - return lemmas\n",
        "def nlp_processing(doc):  \n",
        "    tokens = nlp(doc)\n",
        "    \n",
        "    #print(type(tokens))\n",
        "    # eliminates stop words  and non alpha num and converts all to lower case\n",
        "    terms = [token.lemma_.lower() for token in tokens if not token.is_stop and token.is_alpha]\n",
        "  \n",
        "    return terms\n",
        "\n",
        "# Step 2: extract a list of (token, doc_id) from all documents.\n",
        "# input a list of documents\n",
        "# output: a list of sorted (token, doc_id) tuples\n",
        "def extract_token_doc_id(list_doc):\n",
        "  all_tokens = []\n",
        "  for ind_doc, doc in enumerate(list_doc):\n",
        "    tokens_doc = [(token, ind_doc) for token in nlp_processing(doc)]\n",
        "    all_tokens.extend(tokens_doc)\n",
        "  \n",
        "  # sort by token name \n",
        "  all_tokens = sorted(all_tokens, key = lambda x:x[0])\n",
        "\n",
        "  return all_tokens\n",
        "\n",
        "# Step 3: Extract terms (unique) and document frequency (count tokens)\n",
        "# change this to account only once for a repeated term in the same document\n",
        "# all_tokens list of tuples\n",
        "def doc_freq(all_tokens):\n",
        "  set_all_tokens = set(all_tokens) # remove duplicate token in the same document\n",
        "  dict_doc_freq = {}\n",
        "  for (token, doc) in set_all_tokens:\n",
        "    if token in dict_doc_freq:\n",
        "      dict_doc_freq[token] += 1\n",
        "    else: \n",
        "      dict_doc_freq[token] = 1\n",
        "\n",
        "  # sort by key (term)  \n",
        "  tuples_doc_freq = sorted(dict_doc_freq.items(), key = lambda x: x[0])\n",
        "  \n",
        "  dict_doc_freq = {term:doc_freq for (term, doc_freq) in tuples_doc_freq}\n",
        "  return dict_doc_freq\n",
        "\n",
        "# Step 4: Extract term frequency of each term in each document it appears in\n",
        "# dict_term_freq = {term: {doc1:tf1, doc2:tf2, ...}} # includes only docs that have \n",
        "# non-zero term frequency\n",
        "def term_freq(all_tokens, dict_doc_freq):\n",
        "  dict_term_freq = {term:{} for term in dict_doc_freq.keys()} # initialize dictionary with all unique terms\n",
        "  for (token, doc) in all_tokens:\n",
        "    if doc in dict_term_freq[token]:\n",
        "      dict_term_freq[token][doc] += 1 \n",
        "    else: # if doc is not a key in the dictionary \n",
        "      dict_term_freq[token][doc] = 1\n",
        "  \n",
        "  return dict_term_freq"
      ],
      "metadata": {
        "id": "C1eHgdQEOUTY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1: extract tokens from one document\n",
        "tokens1 = nlp_processing(list_doc[0])\n",
        "tokens1[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNldUR1HYWCg",
        "outputId": "1a03b365-3c34-4982-fde7-102ffbc01b5d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['information',\n",
              " 'retrieval',\n",
              " 'course',\n",
              " 'cover',\n",
              " 'technique',\n",
              " 'search',\n",
              " 'engine',\n",
              " 'find',\n",
              " 'relevant',\n",
              " 'document',\n",
              " 'information',\n",
              " 'relate',\n",
              " 'query',\n",
              " 'topic',\n",
              " 'include',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'extract',\n",
              " 'relevant']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: extract a list of tuples (token, doc id), sorted alphabetically\n",
        "all_tokens = extract_token_doc_id(list_doc)\n",
        "all_tokens[:20]"
      ],
      "metadata": {
        "id": "cSzjdZ1UOXbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8681461e-c4a8-4eea-813a-80eaf38a7b9f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('academic', 4),\n",
              " ('access', 3),\n",
              " ('accord', 3),\n",
              " ('accurate', 1),\n",
              " ('actor', 3),\n",
              " ('addition', 3),\n",
              " ('address', 3),\n",
              " ('adjustment', 4),\n",
              " ('affect', 4),\n",
              " ('agency', 3),\n",
              " ('agree', 2),\n",
              " ('algorithm', 0),\n",
              " ('algorithm', 1),\n",
              " ('algorithm', 1),\n",
              " ('analyze', 1),\n",
              " ('analyze', 1),\n",
              " ('announce', 2),\n",
              " ('anxious', 4),\n",
              " ('anymore', 4),\n",
              " ('application', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract document frequency all_doc_frequency = dict{term:doc frequency }\n",
        "dict_doc_freq = doc_freq(all_tokens)\n",
        "print(\"Document frequency\")\n",
        "list(dict_doc_freq.items())[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZpNmxYYFtsV",
        "outputId": "4c70f1b8-bc4b-4e8d-aa2e-0b8f2fc42820"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document frequency\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('academic', 1),\n",
              " ('access', 1),\n",
              " ('accord', 1),\n",
              " ('accurate', 1),\n",
              " ('actor', 1),\n",
              " ('addition', 1),\n",
              " ('address', 1),\n",
              " ('adjustment', 1),\n",
              " ('affect', 1),\n",
              " ('agency', 1),\n",
              " ('agree', 1),\n",
              " ('algorithm', 2),\n",
              " ('analyze', 1),\n",
              " ('announce', 1),\n",
              " ('anxious', 1),\n",
              " ('anymore', 1),\n",
              " ('application', 1),\n",
              " ('attorney', 1),\n",
              " ('automatic', 1),\n",
              " ('automatically', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: extract document frequency\n",
        "dict_term_freq = term_freq(all_tokens, dict_doc_freq)\n",
        "print(\"Term frequency\")\n",
        "list(dict_term_freq.items())[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5GcFI-xIDCv",
        "outputId": "8f89627f-e90a-4290-d33f-51aebd568585"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term frequency\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('academic', {4: 1}),\n",
              " ('access', {3: 1}),\n",
              " ('accord', {3: 1}),\n",
              " ('accurate', {1: 1}),\n",
              " ('actor', {3: 1}),\n",
              " ('addition', {3: 1}),\n",
              " ('address', {3: 1}),\n",
              " ('adjustment', {4: 1}),\n",
              " ('affect', {4: 1}),\n",
              " ('agency', {3: 1}),\n",
              " ('agree', {2: 1}),\n",
              " ('algorithm', {0: 1, 1: 2}),\n",
              " ('analyze', {1: 2}),\n",
              " ('announce', {2: 1}),\n",
              " ('anxious', {4: 1}),\n",
              " ('anymore', {4: 1}),\n",
              " ('application', {0: 1}),\n",
              " ('attorney', {3: 1}),\n",
              " ('automatic', {0: 1}),\n",
              " ('automatically', {1: 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(items):\n",
        "  sort_items = sorted(items) # sorts tokens alphabetically\n",
        "  count_items = {}\n",
        "  for item in sort_items:\n",
        "    if item in count_items.keys():\n",
        "      count_items[item] += 1\n",
        "    else:\n",
        "      count_items[item] = 1\n",
        "  \n",
        "  # sort by the count, in reverse order\n",
        "  sorted_count_list = sorted(count_items.items(), \n",
        "                            key = lambda x:x[1], reverse = True)\n",
        "  sorted_count_dict = dict(sorted_count_list)\n",
        "  return sorted_count_dict "
      ],
      "metadata": {
        "id": "Djara30UkDLe"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_func(tf_freq_doc):\n",
        "  return np.log(tf_freq_doc)\n",
        "\n",
        "def idf_func(tf_doc_freq, nr_docs):\n",
        "  return np.log((nr_docs+1)/tf_doc_freq)\n",
        "\n",
        "# implement simple tf-idf function \n",
        "# query - a string\n",
        "# nr_doc = number of documents in the collection\n",
        "def tf_idf(query, dict_doc_freq, dict_term_freq, nr_docs):\n",
        "  # nlp processing of query -> tf_query = {terms: non-zero frequency in query}\n",
        "  tokens   = nlp_processing(query)\n",
        "  tf_query = counter(tokens)\n",
        "  sim_query_doc = {} # doc: similarity function \n",
        "  # for each term_q in the tf_query find doc matches in dict_term_freq:\n",
        "  for (term_q, tf_term_q) in tf_query.items():\n",
        "  # for each doc_id in the dict_term_freq[term].keys()\n",
        "    print('term query = ', term_q, 'freq_query', tf_term_q)\n",
        "    for doc in dict_term_freq[term_q].keys(): \n",
        "  #   if doc_id in sim_query_doc: # you found another matching term in the same doc_id\n",
        "      print('\\t doc id', doc)\n",
        "      if doc in sim_query_doc:\n",
        "  #     sim_query_doc[doc_id] += tf_query * tf_func(dict_term_freq[term_q][doc_id])*idf_func(dict_doc_freq[term_q],nr_doc)\n",
        "        tf_doc  = tf_func(dict_term_freq[term_q][doc])\n",
        "        idf_doc = idf_func(dict_doc_freq[term_q], nr_docs)\n",
        "        sim_query_doc[doc] += tf_term_q * tf_doc * idf_doc\n",
        "      else:\n",
        "        tf_doc  = tf_func(dict_term_freq[term_q][doc])\n",
        "        idf_doc = idf_func(dict_doc_freq[term_q], nr_docs)\n",
        "        sim_query_doc[doc] = tf_term_q * tf_doc * idf_doc\n",
        "      print(\"sim_query for doc = \", doc, \"is =\", sim_query_doc)\n",
        "  #         sim_query_doc[doc_id] = tf_func(dict_term_freq[term_q][doc_id])*idf_func(dict_doc_freq[term_q],nr_doc)\n",
        "  # sort sim_query_doc by similarity value of all keys. \n",
        "  return sim_query_doc\n",
        "      \n"
      ],
      "metadata": {
        "id": "azC93ZrXPl_a"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_query = tf_idf(\"information retrieval\", dict_doc_freq, dict_term_freq, len(list_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbKvPat7njgH",
        "outputId": "bfb4eaf8-df73-479d-c997-578b9e67ed70"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "term query =  information freq_query 1\n",
            "\t doc id 0\n",
            "sim_query for doc =  0 is = {0: 1.206948960812582}\n",
            "\t doc id 3\n",
            "sim_query for doc =  3 is = {0: 1.206948960812582, 3: 0.761500010418809}\n",
            "term query =  retrieval freq_query 1\n",
            "\t doc id 0\n",
            "sim_query for doc =  0 is = {0: 1.206948960812582, 3: 0.761500010418809}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_doc_freq['information']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TP5an3znvTG",
        "outputId": "ee350954-23b7-4ae7-d13c-8af27c2d622e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_term_freq['information']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWN5s8BXoB_j",
        "outputId": "e7fddd36-c57a-490d-c07e-d0a7e4b27f9c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3, 3: 2}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_doc_freq['retrieval']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoLYEiLeoIAx",
        "outputId": "9555f90e-a004-4593-a32c-d968ab2f037a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_term_freq['retrieval']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJ7r3YroLJE",
        "outputId": "57a75b26-ce74-49ee-eb60-15806992e7a6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}