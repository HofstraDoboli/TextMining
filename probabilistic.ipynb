{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMM1J5fujptNV/2Y6PMDFPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HofstraDoboli/TextMining_F22/blob/main/probabilistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pMinkwZKvjQ4"
      },
      "outputs": [],
      "source": [
        "list_doc =[ \"Information retrieval course will cover algorithms used in search engines for finding relevant documents or information related to a query. Topics include: natural language processing for extracting relevant terms out of text data, vector space a methods for computing similarity between documents, text classification, and clustering. These techniques are commonly used in applications such as: automatic extraction of summaries out of a long text, extract novel information in a stream of data.\",  \n",
        "\n",
        "\"NLP algorithms are typically based on machine learning algorithms. Instead of hand-coding large sets of rules, NLP can rely on machine learning to automatically learn these rules by analyzing a set of examples (i.e. a large corpus, like a book, down to a collection of sentences), and making a statical inference. In general, the more data analyzed, the more accurate the model will be.\",  \n",
        "\n",
        "\"The Denver Broncos made sure Brandon McManus will be their kicker for the long haul on Monday.  General manager John Elway announced the team and the kicker agreed on a contract extension. NFL Network Insider Ian Rapoport reported, per a source, it's a three-year extension worth $11.254 million with $6 million of it guaranteed. McManus is now the NFL's fourth highest paid kicker.\", \n",
        "\n",
        " \"Equifax, one of the three major credit reporting agencies, handles the data of 820 million consumers and more than 91 million businesses worldwide. Between May and July of this year 143 million people in the U.S. may have had their names, Social Security numbers, birth dates, addresses and even driver's license numbers accessed. In addition, the hack compromised 209,000 people's credit card numbers and personal dispute details for another 182,000 people. What bad actors could do with that information is daunting. This data breach is more confusing than others -- like when Yahoo or Target were hacked, for example -- according to Joel Winston, a former deputy attorney general for New Jersey , whose current law practice focuses on consumer rights litigation, information privacy, and data protection law.\", \n",
        " \n",
        "  \"\"\"Why didn't she text me back yet? She doesn't like me anymore!\" \"There's no way I'm trying out for the team. I suck at basketball\"\"It's not fair that I have a curfew! \"Sound familiar? Parents of tweens and teens often shrug off such anxious and gloomy thinking as normal irritability and moodiness â€” because it is. Still, the beginning of a new school year, with all of the required adjustments, is a good time to consider just how closely the habit of negative, exaggerated \"self-talk\" can affect academic and social success, self-esteem and happiness. Psychological research shows that what we think can have a powerful influence on how we feel emotionally and physically, and on how we behave. Research also shows that our harmful thinking patterns can be changed.\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy   # another tokenizer, lemmatizer (has --> be)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.disable_pipes('parser', 'ner') "
      ],
      "metadata": {
        "id": "AQfy0LkPxMsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f96f78c-8227-4100-9440-4b7d9f41e3eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parser', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: text processing for one document - return lemmas\n",
        "def nlp_processing(doc):  \n",
        "    tokens = nlp(doc)\n",
        "    \n",
        "    #print(type(tokens))\n",
        "    # eliminates stop words  and non alpha num and converts all to lower case\n",
        "    terms = [token.lemma_.lower() for token in tokens if not token.is_stop and token.is_alpha] \n",
        "  \n",
        "    return terms\n",
        "\n",
        "# Step 2: extract a list of (token, doc_id) from all documents.\n",
        "# input a list of documents\n",
        "# output: a list of sorted (token, doc_id) tuples\n",
        "def extract_token_doc_id(list_doc):\n",
        "  all_tokens = []\n",
        "  len_docs = [0]*len(list_doc)\n",
        "  for ind_doc, doc in enumerate(list_doc):\n",
        "    tokens = nlp_processing(doc)\n",
        "    len_docs[ind_doc] = len(tokens)\n",
        "    tokens_doc = [(token, ind_doc) for token in tokens]\n",
        "    all_tokens.extend(tokens_doc)\n",
        "  \n",
        "  # sort by token name \n",
        "  all_tokens = sorted(all_tokens, key = lambda x:x[0])\n",
        "\n",
        "  return all_tokens, len_docs\n",
        "\n",
        "def counter(items):\n",
        "  sort_items = sorted(items) # sorts tokens alphabetically\n",
        "  count_items = {}\n",
        "  for item in sort_items:\n",
        "    if item in count_items.keys():\n",
        "      count_items[item] += 1\n",
        "    else:\n",
        "      count_items[item] = 1\n",
        "  \n",
        "  # sort by the count, in reverse order\n",
        "  sorted_count_list = sorted(count_items.items(), \n",
        "                            key = lambda x:x[1], reverse = True)\n",
        "  sorted_count_dict = dict(sorted_count_list)\n",
        "  return sorted_count_dict \n",
        "\n",
        "\n",
        "# Step 3: Extract terms (unique) and document frequency (count tokens)\n",
        "# change this to account only once for a repeated term in the same document\n",
        "# all_tokens list of tuples\n",
        "def doc_freq(all_tokens):\n",
        "  set_all_tokens = set(all_tokens) # remove duplicate token in the same document\n",
        "  dict_doc_freq = {}\n",
        "  for (token, doc) in set_all_tokens:\n",
        "    if token in dict_doc_freq:\n",
        "      dict_doc_freq[token] += 1\n",
        "    else: \n",
        "      dict_doc_freq[token] = 1\n",
        "\n",
        "  # sort by key (term)  \n",
        "  tuples_doc_freq = sorted(dict_doc_freq.items(), key = lambda x: x[0])\n",
        "  \n",
        "  dict_doc_freq = {term:doc_freq for (term, doc_freq) in tuples_doc_freq}\n",
        "  return dict_doc_freq\n",
        "\n",
        "# Step 4: Extract term frequency of each term in each document it appears in\n",
        "# dict_term_freq = {term: {doc1:tf1, doc2:tf2, ...}} # includes only docs that have \n",
        "# non-zero term frequency\n",
        "def term_freq(all_tokens, dict_doc_freq):\n",
        "  dict_term_freq = {term:{} for term in dict_doc_freq.keys()} # initialize dictionary with all unique terms\n",
        "  for (token, doc) in all_tokens:\n",
        "    if doc in dict_term_freq[token]:\n",
        "      dict_term_freq[token][doc] += 1 \n",
        "    else: # if doc is not a key in the dictionary \n",
        "      dict_term_freq[token][doc] = 1\n",
        "  \n",
        "  return dict_term_freq"
      ],
      "metadata": {
        "id": "AR1saorIxRfd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp processing on all docs, extract len_docs\n",
        "list_token_doc, len_docs = extract_token_doc_id(list_doc)\n",
        "print(list_token_doc[:5])\n",
        "print(len_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XAtHFpI5d9Z",
        "outputId": "45067149-e091-46d8-8510-d6aa0afb460a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('academic', 4), ('access', 3), ('accord', 3), ('accurate', 1), ('actor', 3)]\n",
            "[45, 37, 38, 70, 59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the P(w|collection) = count of a word in the collection/sum length docs\n",
        "len_collection = sum(len_docs)\n",
        "all_tokens = [t for (t,d) in list_token_doc]\n",
        "dict_collection = counter(all_tokens)\n",
        "print(list(dict_collection.items())[:5])\n",
        "print(list(dict_collection.items())[-5:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcvGvCqJ5zQk",
        "outputId": "a7b105de-9f6e-4ea9-c786-1d180579fba7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('information', 5), ('million', 5), ('datum', 4), ('text', 4), ('algorithm', 3)]\n",
            "[('way', 1), ('winston', 1), ('worldwide', 1), ('worth', 1), ('yahoo', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_term_freq = term_freq(list_token_doc, dict_count)\n",
        "print(list(dict_term_freq.items())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuElP9pB7wxz",
        "outputId": "90abe212-dd40-44b9-8d30-3f38f73a3454"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('information', {0: 3, 3: 2}), ('million', {2: 2, 3: 3}), ('datum', {0: 2, 1: 1, 3: 1}), ('text', {0: 3, 4: 1}), ('algorithm', {0: 1, 1: 2})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jm similarity method\n",
        "import numpy as np\n",
        "\n",
        "def jm_similarity(query, len_docs, dict_freq_collection, dict_term_freq,\n",
        "                  lam = 0.2):\n",
        "  # process the query - > word and count in the query\n",
        "  query_terms = nlp_processing(query)\n",
        "  dict_freq_query = counter(query_terms)\n",
        "  similarity = {}\n",
        "  # find the set of docs with at least 1 query word\n",
        "  len_all = sum(len_docs)\n",
        "  words_query = dict_freq_query.keys()\n",
        "  docs = [] \n",
        "  for w in words_query:\n",
        "    docs.extend(dict_term_freq[w].keys())\n",
        "  docs = set(docs)\n",
        "\n",
        "  similarity = {d:0 for d in docs}\n",
        "  # for each word in the query\n",
        "  for w in dict_freq_query.keys():\n",
        "  #   for each doc the word appears in\n",
        "    if w not in dict_freq_collection: # if word does not appear in the collection skip over it\n",
        "      continue\n",
        "    # check the documents w appears in\n",
        "    for d in docs:\n",
        "      # compute P(w|d) = (1-lambda) * c(w,d)/len(d) + lambda*c(w,C)/sum(len)\n",
        "      c_w_d = 0\n",
        "      if d in dict_term_freq[w]:\n",
        "        c_w_d = dict_term_freq[w][d]\n",
        "\n",
        "      prob_w_d = (1-lam)* c_w_d/len_docs[d] + (lam*dict_freq_collection[w])/len_all\n",
        "\n",
        "      # add to the similarity of (q,d) =  c(w,q) * log(P(w|d)\n",
        "      similarity[d] += dict_freq_query[w] * np.log(prob_w_d)\n",
        "\n",
        "  # sort the similarity by value \n",
        "  sorted_sim = sorted(similarity.items(), key = lambda x: x[1], reverse = True)\n",
        "  return sorted_sim"
      ],
      "metadata": {
        "id": "6gaJ9vIz8n4F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_doc = jm_similarity(\"information retrieval\", len_docs, dict_collection, dict_term_freq, 0.2)\n",
        "ranked_doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T1PQeZ8B-1c",
        "outputId": "0ebf1954-8b66-4e16-efc5-acf704d4c653"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, -6.844209169607428), (3, -10.743516315689313)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}