{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_qH5WND5vDV6evgj4oV-MH9u_ItGBlwK",
      "authorship_tag": "ABX9TyNeTx+395F/IggI1r3tpem9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HofstraDoboli/TextMining/blob/main/page_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsYx9zkD9F6R",
        "outputId": "8dd75104-5b8c-4181-88f7-19fe7e5d3f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [1 1 0 0]]\n",
            "[2 1 1 2]\n",
            "[[0.  0.  0.5 0.5]\n",
            " [1.  0.  0.  0. ]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]]\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 4\n",
        "mat_con = np.array([(0, 0, 1, 1), (1,0,0,0), (0,1,0,0), (1,1,0,0)])\n",
        "sum_con = mat_con.sum(axis = 1)   # sum over the rows\n",
        "print(mat_con)\n",
        "print(sum_con)\n",
        "#sum_con[sum_con == 0] = 1\n",
        "#print(sum_con)\n",
        "prob_mat_con = np.divide(mat_con,sum_con[:,np.newaxis])  # add a new axis to sum_con\n",
        "print(prob_mat_con)\n",
        "random_mat = (1/n) * np.ones((n,n))   # creates a matrix of ones\n",
        "print(random_mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize rank\n",
        "p0 = 0.1 * np.ones((4,1)) # np.random.rand(4,1)\n",
        "print(p0)\n",
        "# initialize an array with random values - uniform distributed 0 to 1\n",
        "\n",
        "p_current = np.copy(p0)\n",
        "alpha = 0.3\n",
        "\n",
        "A_mat = np.transpose(alpha * random_mat + (1- alpha) * prob_mat_con)\n",
        "print(A_mat)\n",
        "p_new  = A_mat @ p_current  # (4,4) X(4,1)= (4,1)\n",
        "iter = 1\n",
        "while np.any(np.absolute(p_current - p_new) > 0.001):\n",
        "    p_current = p_new\n",
        "    p_new = A_mat @ p_current\n",
        "    print(\"\\nIteration\", iter)\n",
        "    print(\"Previous value\\n\", p_current)\n",
        "    print(\"New value\\n\", p_new)\n",
        "    iter += 1\n",
        "\n",
        "print(p_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtzjxB0R9Mg6",
        "outputId": "192e24c9-c06b-4a1a-e8f7-65679d0d6c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1]\n",
            " [0.1]\n",
            " [0.1]\n",
            " [0.1]]\n",
            "[[0.075 0.775 0.075 0.425]\n",
            " [0.075 0.075 0.775 0.425]\n",
            " [0.425 0.075 0.075 0.075]\n",
            " [0.425 0.075 0.075 0.075]]\n",
            "\n",
            "Iteration 1\n",
            "Previous value\n",
            " [[0.135]\n",
            " [0.135]\n",
            " [0.065]\n",
            " [0.065]]\n",
            "New value\n",
            " [[0.14725]\n",
            " [0.09825]\n",
            " [0.07725]\n",
            " [0.07725]]\n",
            "\n",
            "Iteration 2\n",
            "Previous value\n",
            " [[0.14725]\n",
            " [0.09825]\n",
            " [0.07725]\n",
            " [0.07725]]\n",
            "New value\n",
            " [[0.1258125]\n",
            " [0.1111125]\n",
            " [0.0815375]\n",
            " [0.0815375]]\n",
            "\n",
            "Iteration 3\n",
            "Previous value\n",
            " [[0.1258125]\n",
            " [0.1111125]\n",
            " [0.0815375]\n",
            " [0.0815375]]\n",
            "New value\n",
            " [[0.13631687]\n",
            " [0.11561437]\n",
            " [0.07403437]\n",
            " [0.07403437]]\n",
            "\n",
            "Iteration 4\n",
            "Previous value\n",
            " [[0.13631687]\n",
            " [0.11561437]\n",
            " [0.07403437]\n",
            " [0.07403437]]\n",
            "New value\n",
            " [[0.13684209]\n",
            " [0.10773609]\n",
            " [0.07771091]\n",
            " [0.07771091]]\n",
            "\n",
            "Iteration 5\n",
            "Previous value\n",
            " [[0.13684209]\n",
            " [0.10773609]\n",
            " [0.07771091]\n",
            " [0.07771091]]\n",
            "New value\n",
            " [[0.13261408]\n",
            " [0.11159645]\n",
            " [0.07789473]\n",
            " [0.07789473]]\n",
            "\n",
            "Iteration 6\n",
            "Previous value\n",
            " [[0.13261408]\n",
            " [0.11159645]\n",
            " [0.07789473]\n",
            " [0.07789473]]\n",
            "New value\n",
            " [[0.13538067]\n",
            " [0.11178947]\n",
            " [0.07641493]\n",
            " [0.07641493]]\n",
            "\n",
            "Iteration 7\n",
            "Previous value\n",
            " [[0.13538067]\n",
            " [0.11178947]\n",
            " [0.07641493]\n",
            " [0.07641493]]\n",
            "New value\n",
            " [[0.13499785]\n",
            " [0.11023568]\n",
            " [0.07738324]\n",
            " [0.07738324]]\n",
            "\n",
            "Iteration 8\n",
            "Previous value\n",
            " [[0.13499785]\n",
            " [0.11023568]\n",
            " [0.07738324]\n",
            " [0.07738324]]\n",
            "New value\n",
            " [[0.13424911]\n",
            " [0.1112524 ]\n",
            " [0.07724925]\n",
            " [0.07724925]]\n",
            "\n",
            "Iteration 9\n",
            "Previous value\n",
            " [[0.13424911]\n",
            " [0.1112524 ]\n",
            " [0.07724925]\n",
            " [0.07724925]]\n",
            "New value\n",
            " [[0.13491392]\n",
            " [0.11111171]\n",
            " [0.07698719]\n",
            " [0.07698719]]\n",
            "[[0.13491392]\n",
            " [0.11111171]\n",
            " [0.07698719]\n",
            " [0.07698719]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LGpQ9CMcLtl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/TextMining/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p1NOH0KMDJL",
        "outputId": "307721da-f1d3-44a4-e569-3bcc28505b00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TextMining\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutTafos_ydN",
        "outputId": "75b6362b-f7be-4a0c-db53-8d28de1ea1ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2025.8.3)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=af060cf6c847a353c09e94d2815a11dd2f6077a13fa2484903a0711585eca86d\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import networkx as nx\n",
        "\n",
        "wiki = wikipediaapi.Wikipedia(language='en', user_agent='MyWikipediaApp/1.0') # Added user agent\n",
        "category = wiki.page(\"Category:Information retrieval\") # starting web page\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for title in category.categorymembers.keys(): # for all links in the starting web page\n",
        "    page = wiki.page(title)\n",
        "    for link in page.links.keys(): # extract titles for each link in the starting web page\n",
        "        G.add_edge(title, link)\n",
        "        #subpage = wiki.page(link)\n",
        "        #for sublink in subpage.links.keys():\n",
        "        #  G.add_edge(link, sublink)\n",
        "\n",
        "print(G)\n",
        "print(\"Number of nodes:\", G.number_of_nodes())\n",
        "print(\"Number of edges:\", G.number_of_edges())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpjh6ewv_u6R",
        "outputId": "1cbd7969-9976-44d4-dd25-2239fbe8add5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 644 nodes and 714 edges\n",
            "Number of nodes: 644\n",
            "Number of edges: 714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e96e864",
        "outputId": "2ab35ab5-ea0e-4927-9e81-393eb1016460"
      },
      "source": [
        "# Save the graph to a GraphML file\n",
        "nx.write_graphml(G, \"information_retrieval_graph.graphml\")\n",
        "print(\"Graph saved\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Load the graph from the GraphML file\n",
        "G = nx.read_graphml(\"information_retrieval_graph.graphml\")\n",
        "print(\"Graph loaded successfully.\")\n",
        "print(G)\n",
        "print(\"Number of nodes in loaded graph:\", G.number_of_nodes())\n",
        "print(\"Number of edges in loaded graph:\", G.number_of_edges())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zVBVI94LcYU",
        "outputId": "29c97239-d3a8-48db-fb6e-4c45ecceaed2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph loaded successfully.\n",
            "DiGraph with 644 nodes and 714 edges\n",
            "Number of nodes in loaded graph: 644\n",
            "Number of edges in loaded graph: 714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pagerank(graph, alpha=0.85, tol=1e-6, max_iter=100):\n",
        "    \"\"\"\n",
        "    Implements the PageRank algorithm on a NetworkX graph.\n",
        "\n",
        "    Args:\n",
        "        graph: A NetworkX graph.\n",
        "        alpha: The damping factor (probability of following a link).\n",
        "        tol: The tolerance for convergence.\n",
        "        max_iter: The maximum number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of PageRank scores for each node.\n",
        "    \"\"\"\n",
        "    nodes = list(graph.nodes())\n",
        "    n = len(nodes)\n",
        "    # Create a mapping from node to index\n",
        "    node_to_index = {node: i for i, node in enumerate(nodes)}\n",
        "\n",
        "    # Build the transition matrix T from the adjacency matrix\n",
        "    # Handle dangling nodes (nodes with no outgoing edges)\n",
        "    out_degree = np.array([graph.out_degree(node) for node in nodes])\n",
        "    # Replace 0 out_degree with 1 to avoid division by zero, these nodes will distribute their rank equally\n",
        "    out_degree[out_degree == 0] = 1\n",
        "\n",
        "    # Create the transition matrix\n",
        "    # The original graph adjacency matrix is G, where G[i,j] = 1 if there is an edge from i to j\n",
        "    # The transition matrix T has T[j,i] = 1/out_degree[i] if there is an edge from i to j\n",
        "    # We need to build T from the adjacency matrix of the graph\n",
        "    adj_matrix = nx.adjacency_matrix(graph, nodelist=nodes).todense().T # Transpose to get correct transitions\n",
        "\n",
        "    # Normalize the adjacency matrix by out_degree\n",
        "    # Use broadcasting to divide each column by the out_degree of the source node (column sum before transpose)\n",
        "    # We need to divide by out-degree of the source node, which is the sum of rows in the non-transposed matrix\n",
        "    # Let's re-calculate transition matrix directly based on definition\n",
        "    T = np.zeros((n, n))\n",
        "    for u in graph.nodes():\n",
        "        for v in graph.successors(u):\n",
        "            T[node_to_index[v], node_to_index[u]] = 1.0 / graph.out_degree(u)\n",
        "\n",
        "    # Create the Google matrix G\n",
        "    # Handling dangling nodes: distribute their rank equally among all nodes\n",
        "    dangling_nodes_index = [i for i, node in enumerate(nodes) if graph.out_degree(node) == 0]\n",
        "    A_prime = alpha * T\n",
        "    # Add contribution from dangling nodes and random jumps\n",
        "    A_prime[:, dangling_nodes_index] += alpha * (1/n)\n",
        "    G_matrix = A_prime + (1 - alpha) * (1/n)\n",
        "\n",
        "\n",
        "    # Initialize PageRank vector\n",
        "    p_current = np.ones((n, 1)) / n\n",
        "\n",
        "    # Iterative power method\n",
        "    for i in range(max_iter):\n",
        "        p_new = G_matrix @ p_current\n",
        "        if np.linalg.norm(p_new - p_current, ord=1) < tol:\n",
        "            print(f\"PageRank converged after {i+1} iterations.\")\n",
        "            break\n",
        "        p_current = p_new\n",
        "    else:\n",
        "        print(f\"PageRank did not converge after {max_iter} iterations.\")\n",
        "\n",
        "    # Convert PageRank vector to a dictionary with node names\n",
        "    pagerank_scores = {nodes[i]: p_current[i, 0] for i in range(n)}\n",
        "\n",
        "    return pagerank_scores\n",
        "\n",
        "ranked_scores = dict(sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)[:20])\n",
        "# Print the PageRank scores\n",
        "print(\"PageRank scores:\")\n",
        "for node, score in ranked_scores.items():\n",
        "    print(f\"Node: {node}, PageRank Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAb3MhafVYJB",
        "outputId": "fd185cf7-f630-4b27-81ca-dcad027074e6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank scores:\n",
            "Node: Information retrieval, PageRank Score: 0.0035\n",
            "Node: Help:Categories, PageRank Score: 0.0028\n",
            "Node: Web search engine, PageRank Score: 0.0022\n",
            "Node: ISBN (identifier), PageRank Score: 0.0020\n",
            "Node: Information seeking, PageRank Score: 0.0020\n",
            "Node: Citation index, PageRank Score: 0.0019\n",
            "Node: Data structure, PageRank Score: 0.0018\n",
            "Node: Inverted index, PageRank Score: 0.0018\n",
            "Node: Category:Bibliographic databases and indexes, PageRank Score: 0.0018\n",
            "Node: Category:Citation metrics, PageRank Score: 0.0018\n",
            "Node: Information behavior, PageRank Score: 0.0018\n",
            "Node: Search engine, PageRank Score: 0.0018\n",
            "Node: Library classification, PageRank Score: 0.0018\n",
            "Node: Semantic network, PageRank Score: 0.0018\n",
            "Node: Help:Disambiguation, PageRank Score: 0.0018\n",
            "Node: Knowledge representation, PageRank Score: 0.0018\n",
            "Node: Ontology (computer science), PageRank Score: 0.0018\n",
            "Node: Doi (identifier), PageRank Score: 0.0017\n",
            "Node: Library and information science, PageRank Score: 0.0017\n",
            "Node: Wikipedia:Please clarify, PageRank Score: 0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the PageRank algorithm\n",
        "pagerank_scores = nx.pagerank(loaded_graph)\n",
        "\n",
        "ranked_scores = dict(sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)[:20])\n",
        "# Print the PageRank scores\n",
        "print(\"PageRank scores:\")\n",
        "for node, score in ranked_scores.items():\n",
        "    print(f\"Node: {node}, PageRank Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Q8w2kVMy2w",
        "outputId": "e07227ed-b7c1-469b-fada-248c815e9f51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank scores:\n",
            "Node: Information retrieval, PageRank Score: 0.0035\n",
            "Node: Help:Categories, PageRank Score: 0.0028\n",
            "Node: Web search engine, PageRank Score: 0.0022\n",
            "Node: ISBN (identifier), PageRank Score: 0.0020\n",
            "Node: Information seeking, PageRank Score: 0.0020\n",
            "Node: Citation index, PageRank Score: 0.0019\n",
            "Node: Data structure, PageRank Score: 0.0018\n",
            "Node: Inverted index, PageRank Score: 0.0018\n",
            "Node: Category:Bibliographic databases and indexes, PageRank Score: 0.0018\n",
            "Node: Category:Citation metrics, PageRank Score: 0.0018\n",
            "Node: Information behavior, PageRank Score: 0.0018\n",
            "Node: Search engine, PageRank Score: 0.0018\n",
            "Node: Library classification, PageRank Score: 0.0018\n",
            "Node: Semantic network, PageRank Score: 0.0018\n",
            "Node: Help:Disambiguation, PageRank Score: 0.0018\n",
            "Node: Knowledge representation, PageRank Score: 0.0018\n",
            "Node: Ontology (computer science), PageRank Score: 0.0018\n",
            "Node: Doi (identifier), PageRank Score: 0.0017\n",
            "Node: Library and information science, PageRank Score: 0.0017\n",
            "Node: Wikipedia:Please clarify, PageRank Score: 0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the nodes and print their names\n",
        "print(\"Nodes in the graph:\")\n",
        "count = 0\n",
        "for node in G.nodes():\n",
        "    if count < 10:\n",
        "      print(count, node)\n",
        "      count += 1\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCE2S2LWV0FC",
        "outputId": "10764cc1-3d36-470a-eaf2-83f1bafd9391"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes in the graph:\n",
            "0 Information retrieval\n",
            "1 1890 US census\n",
            "2 1790 United States census\n",
            "3 1800 United States census\n",
            "4 1810 United States census\n",
            "5 1820 United States census\n",
            "6 1830 United States census\n",
            "7 1840 United States census\n",
            "8 1850 United States census\n",
            "9 1860 United States census\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print edge information for each node\n",
        "print(\"Edge information for the first 10 nodes:\")\n",
        "count = 0\n",
        "for node in G.nodes():\n",
        "    if count < 10:\n",
        "        print(f\"\\nNode: {node}\")\n",
        "        print(f\"  Successors (outgoing edges): {list(G.successors(node))}\")\n",
        "        print(f\"  Predecessors (incoming edges): {list(G.predecessors(node))}\")\n",
        "        count += 1\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmVOSL4RWr-a",
        "outputId": "e347e765-cc88-4e55-fe2b-53afa2d3f856"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge information for the first 10 nodes:\n",
            "\n",
            "Node: Information retrieval\n",
            "  Successors (outgoing edges): ['1890 US census', '3D retrieval', 'Adversarial information retrieval', 'Allen Kent', 'AltaVista', 'Alvin Weinberg', 'Apache Lucene', 'Apache Solr', 'ArXiv (identifier)', 'As We May Think', 'Association for Computing Machinery', 'Atlantic Monthly', 'Automatic summarization', 'BERT (language model)', \"Bayes' theorem\", 'Bibliometrics', 'Bill Maron', 'Binary Independence Model', 'C. J. van Rijsbergen', 'CERN', 'Calvin Mooers', 'Case Western Reserve University', 'Categorization', 'Censorship', 'Citation index', 'CiteSeerX (identifier)', 'Co-occurrence', 'Collaborative information seeking', 'Communications of the ACM', 'Compound term processing', 'Computational linguistics', 'Computer data storage', 'Computer memory', 'Computing', 'Conference on Information and Knowledge Management', 'Controlled vocabulary', 'Cornelis J. van Rijsbergen', 'Cross-language information retrieval', 'Cultural studies', 'Cyril W. Cleverdon', 'Data mining', 'Data modeling', 'Data retrieval', 'Database', 'Desk Set', 'Desktop search', 'Digital libraries', 'Dimension reduction', 'Divergence-from-randomness model', 'Document classification', 'Doi (identifier)', 'Don Swanson', 'Donald B. Crouch', 'Elasticsearch', 'Emanuel Goldberg', 'Enterprise search', 'Eugene Garfield', 'European Conference on Information Retrieval', 'European Summer School in Information Retrieval', 'Evaluation measures (information retrieval)', 'Extended Boolean model', 'F. Wilfrid Lancaster', 'Federated search', 'Full-text search', 'Fuzzy retrieval', 'Generalized vector space model', 'Geographic information retrieval', 'Gerard Salton', 'Gerard Salton Award', 'Google', 'Ground truth', 'Hans Peter Luhn', 'Hdl (identifier)', 'Herman Hollerith', 'Hierarchic clustering', 'Human–computer information retrieval', 'Hypertext', 'ISBN (identifier)', 'ISSN (identifier)', 'Ill-posed', 'Image retrieval', 'Independence (mathematical logic)', 'Informatics', 'Information Retrieval Facility', 'Information access', 'Information architecture', 'Information behavior', 'Information extraction', 'Information filtering', 'Information management', 'Information needs', 'Information overload', 'Information retrieval applications', 'Information science', 'Information seeking', 'Information society', 'Information system', 'Information technology', 'Intellectual freedom', 'Intellectual property', 'International World Wide Web Conference', 'J. C. R. Licklider', 'JASIS', 'Jacquard loom', 'Joseph Marie Jacquard', 'Karen Spärck Jones', 'Karen Spärck Jones Award', 'Keypunch', 'Knowledge organization', 'Knowledge visualization', 'Language model', 'Larry Page', 'Latent Dirichlet allocation', 'Latent semantic analysis', 'Latent semantic indexing', 'Learning to rank', 'Legal information retrieval', 'Lemur Project', 'Library and information science', 'Library classification', 'MEDLARS', 'MIT', 'Melvin Earl Maron', 'Memory', 'Metadata', 'Microsoft', 'Microsoft Bing', 'Mind maps', 'Mobile search', 'Multi-document summarization', 'Multimedia information retrieval', 'Music information retrieval', 'National Bureau of Standards', 'National Institute of Standards and Technology', 'Natural language user interface', 'Nearest centroid classifier', 'Nicholas J. Belkin', 'Nicholas Jardine', 'Ontology (information science)', 'Orthogonality', 'Outline of information science', 'PageRank', 'Pearl growing', 'Personal information management', 'Philosophy of information', 'Precision and recall', 'Preservation (library and archival science)', 'Privacy', 'Probabilistic relevance model', 'Probabilistic relevance model (BM25)', 'Punched cards', 'Quantum information science', 'Query understanding', 'Question answering', 'Ranking (information retrieval)', 'Recommender systems', 'Relevance (information retrieval)', 'Relevance feedback', 'Retrievability', 'Ricardo Baeza-Yates', 'Robert M. Hayes (information scientist)', 'Robert R. Korfhage', 'S2CID (identifier)', 'SMART Information Retrieval System', 'SPLADE', 'Scalability', 'Science', 'Science and technology studies', 'Search engine indexing', 'Search engines', 'Semantic Web', 'Sergey Brin', 'Set (mathematics)', 'Site search', 'Sketch Engine', 'Social information seeking', 'Social search', 'Software engineering', 'Spam filtering', 'Special Interest Group on Information Retrieval', 'Sphinx (search engine)', 'Standard Boolean model', 'Subject indexing', 'Tabulating machine', 'Taxonomy', 'Temporal information retrieval', 'Term Discrimination', 'Terrier Search Engine', 'Text Retrieval Conference', 'Text corpora', 'Tf–idf', 'Theodor Nelson', 'Tim Berners-Lee', 'Tony Kent Strix award', 'Topic-based vector space model', 'Uncertain inference', 'Univac', 'Vannevar Bush', 'Vector space model', 'Vertical search', 'Video retrieval', 'Wayback Machine', 'Web mining', 'Web search engine', 'Wikipedia', 'World Wide Web', 'XML retrieval', 'Xapian', 'Yahoo! Inc. (1995–2017)', 'Wikipedia:Please clarify', 'Wikipedia:Verifiability', 'Template:Cite journal', 'Template:Cite web', 'Template:Information science', 'Template talk:Information science', 'Help:Authority control', 'Help:CS1 errors', 'Help:Maintenance template removal', 'Help:Referencing for beginners', 'Category:CS1 maint: location']\n",
            "  Predecessors (incoming edges): ['Geographic information retrieval', 'Information needs', 'Information search', 'Postings list', 'Category:Information retrieval evaluation']\n",
            "\n",
            "Node: 1890 US census\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: 3D retrieval\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: Adversarial information retrieval\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: Allen Kent\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: AltaVista\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: Alvin Weinberg\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: Apache Lucene\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: Apache Solr\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval']\n",
            "\n",
            "Node: ArXiv (identifier)\n",
            "  Successors (outgoing edges): []\n",
            "  Predecessors (incoming edges): ['Information retrieval', 'Wiener connector']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of successors for each node\n",
        "print(\"Number of successors for each node:\")\n",
        "count = 0\n",
        "for node in G.nodes():\n",
        "    if count < 10:\n",
        "        print(f\"Node: {node}, Number of Successors: {G.out_degree(node)}\")\n",
        "        print(f\"Node: {node}, Number of Predecessors: {G.in_degree(node)}\")\n",
        "        count += 1\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLGWCOm9XVJy",
        "outputId": "75e7a4d0-12be-443b-bafc-df58f710828f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successors for each node:\n",
            "Node: Information retrieval, Number of Successors: 220\n",
            "Node: Information retrieval, Number of Predecessors: 5\n",
            "Node: 1890 US census, Number of Successors: 0\n",
            "Node: 1890 US census, Number of Predecessors: 1\n",
            "Node: 3D retrieval, Number of Successors: 0\n",
            "Node: 3D retrieval, Number of Predecessors: 1\n",
            "Node: Adversarial information retrieval, Number of Successors: 0\n",
            "Node: Adversarial information retrieval, Number of Predecessors: 1\n",
            "Node: Allen Kent, Number of Successors: 0\n",
            "Node: Allen Kent, Number of Predecessors: 1\n",
            "Node: AltaVista, Number of Successors: 0\n",
            "Node: AltaVista, Number of Predecessors: 1\n",
            "Node: Alvin Weinberg, Number of Successors: 0\n",
            "Node: Alvin Weinberg, Number of Predecessors: 1\n",
            "Node: Apache Lucene, Number of Successors: 0\n",
            "Node: Apache Lucene, Number of Predecessors: 1\n",
            "Node: Apache Solr, Number of Successors: 0\n",
            "Node: Apache Solr, Number of Predecessors: 1\n",
            "Node: ArXiv (identifier), Number of Successors: 0\n",
            "Node: ArXiv (identifier), Number of Predecessors: 2\n"
          ]
        }
      ]
    }
  ]
}